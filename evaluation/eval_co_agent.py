# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

import os
import random
import time
import json
import asyncio
import subprocess
import argparse
import logging
from tavily import TavilyClient
import tiktoken
from transformers import AutoTokenizer

# --- å…³é”®ä¿®æ”¹ï¼šä» LLM_API å¯¼å…¥é…ç½®å’Œå‡½æ•° ---
from LLM_API import get_llm_response, MODEL_MAPPING

logging.disable(logging.CRITICAL)

# å…¨å±€å˜é‡å ä½
MODEL_NAME = None
my_output_dir = None
MAX_ROUNDS = None
MODEL_TYPE = None
TOOL_PRICING = None
# åŠ è½½å·¥å…·å®šä¹‰
with open('tools.json') as f:
    raw_tools = json.load(f)

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")
# --- æœç´¢å®¢æˆ·ç«¯åˆå§‹åŒ– (åªåœ¨è¿™é‡Œç”¨) ---
os.environ["TAVILY_API_KEY"] = "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"
tavily_client = TavilyClient(api_key=os.environ.get("TAVILY_API_KEY"))

# å·¥å…·å®šä¹‰å­—å…¸
ALL_TOOLS = {
    "enhance_reasoning": {'model': ["reasoner-1", "reasoner-2", "reasoner-3"]},
    "answer": {'model': ["answer-math-1", "answer-math-2", "answer-1", "answer-2", "answer-3", "answer-4"]},
    "search": {"model": ["search-1", "search-2", "search-3"]},
}

def cut_seq(seq,l):
    if len(seq)==0:
        return {
            'effective_length': 0,
            'string_after_cut': ''
        }
    token_ids = tokenizer(seq)['input_ids']
    rs = tokenizer.batch_decode(token_ids[-l:], skip_special_tokens=True)
    return {
        'effective_length': len(token_ids),
        'string_after_cut': ''.join(rs)
    }

def call_tool(arguments):
    """
    é‡æ„åçš„å·¥å…·æ‰§è¡Œå‡½æ•°ã€‚
    """
    tool_name = arguments['tool']
    
    # ---------------------------
    # å·¥å…· 1: Enhance Reasoning (å†™ä»£ç å¹¶æ‰§è¡Œ)
    # ---------------------------
    if tool_name == 'enhance_reasoning':
        prompt = arguments['context_str'].strip() + '\n\n'
        prompt += f"Question: {arguments['problem']}\nInstead of directly answering the question, please write additional python code that will give intermidiate results after execution. Wrap the code within ```python and ```. The code should be self-contained with all the import and initialization."
        
        # è°ƒç”¨ API ç”Ÿæˆä»£ç 
        response = get_llm_response(
            model_alias=arguments['model'],
            messages=[{"role": "user", "content": prompt}],
            temperature=1
        )
        
        if isinstance(response, str): # API æŠ¥é”™
            arguments['generated_code'] = ''
            arguments['exec_result'] = f"Error generating code: {response}"
            return arguments

        content = response.choices[0].message.content
        try:
            generated_code = content.split('```python')[-1].split('```')[0].strip()
        except:
            generated_code = ""
            
        if not generated_code:
            arguments['generated_code'] = ""
            arguments['exec_result'] = "No code found in response."
            return arguments

        # æœ¬åœ°æ‰§è¡Œä»£ç 
        arguments['generated_code'] = generated_code
        code_path = os.path.join(arguments['cur_output_dir'], f'exec_code_{arguments["id"]}.py')
        with open(code_path, 'w', encoding='utf-8') as f:
            f.write(generated_code)
            
        try:
            exec_result = subprocess.run(
                ['python', code_path], 
                timeout=30, 
                capture_output=True, 
                text=True
            )
            if exec_result.stdout and len(exec_result.stdout.strip()) > 0:
                final_output = exec_result.stdout
            elif exec_result.stderr and len(exec_result.stderr.strip()) > 0:
                final_output = f"Execution Error:\n{exec_result.stderr}"
            else:
                final_output = "Code executed successfully but printed nothing (stdout is empty)."

            arguments['exec_result'] = final_output
            with open(os.path.join(arguments['cur_output_dir'],f'exec_out_{arguments["id"]}.txt'),'w') as f:
                f.write(final_output)
        except Exception as e:
            arguments['exec_result'] = f"Execution Error: {str(e)}"
            
        return arguments

    # ---------------------------
    # å·¥å…· 2: Search (æœç´¢)
    # ---------------------------
    elif tool_name == 'search':
        prompt = arguments['context_str'].strip()+'\n\n'
        prompt += f"Question: {arguments['problem']}\nInstead of directly answering the question, please write a query to search for a piece of relevant and missing information. The query should be a few key words about the information to search or a short sentence. Wrap the query within <query> and </query>."        
        # ä¿®æ­£ï¼šä¹‹å‰è¿™é‡Œæ‹¼å†™é”™è¯¯å†™æˆäº† get_llm_responsel
        response = get_llm_response(
            model_alias=arguments['model'],  
            messages=[{"role": "user", "content": prompt}]
        )
        
        query = arguments['problem']
        if not isinstance(response, str):
            content = response.choices[0].message.content 
            # ç®€å•å°è¯•æå– <query>ï¼Œå¦‚æœæ¨¡å‹æ²¡éµå¾ªï¼Œå°±ç”¨å…¨æ–‡
            if "<query>" in content:
                query = content.split('<query>')[-1].split('</query>')[0]
            else:
                query = content

        # è°ƒç”¨ Tavily
        try:
            search_result = tavily_client.search(query=query[:300], max_results=10)
            contents = [res['content'] for res in search_result['results']]
        except Exception as e:
            print(f"Search API Error: {e}")
            contents = []

        arguments['query'] = query
        arguments['search_results_data'] = contents
        return arguments

    # ---------------------------
    # å·¥å…· 3: Answer (æœ€ç»ˆå›ç­”)
    # ---------------------------
    elif tool_name == 'answer':
        prompt = arguments['context_str'].strip() + '\n\nProblem:\n' + arguments['problem']
        
        response = get_llm_response(
            model_alias=arguments['model'],
            messages=[
                {"role": "system", "content": "Please reason step by step, and put your final answer within \\boxed{}."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2
        )
        
        if isinstance(response, str):
            print(f"!!! Answer API Error: {response}") # åœ¨æ§åˆ¶å°æ‰“å°å…·ä½“é”™è¯¯
            # æˆ–è€…å°†å…¶å†™å…¥ arguments ä»¥ä¾¿åœ¨ json ä¸­çœ‹åˆ°
            arguments['pred'] = ''
            arguments['response'] = f'!!! Answer API Error: {response}'
            arguments['correctness'] = False
            return arguments

        response_str = response.choices[0].message.content
        arguments['response'] = response_str
        
        if '\\boxed{' in response_str:
            pred = response_str.split('\\boxed{')[-1].split('}')[:-1]
            pred = '}'.join(pred).strip()
        else:
            pred = ""
        arguments['pred'] = pred

        # åˆ¤åˆ†é€»è¾‘
        reference = arguments['answer']
        if pred.lower() == str(reference).lower():
            correctness = True
        else:
            eval_prompt = (
                f"Question: {arguments['problem']}\n"
                f"Student Answer: {pred}\n"
                f"Reference Answer: {reference}\n"
                "Assume reference is correct. Is student answer correct? Output <correct>True</correct> or <correct>False</correct>."
            )
            judge_resp = get_llm_response("answer-1", [{"role": "user", "content": eval_prompt}])
            
            if isinstance(judge_resp, str):
                correctness = False
            else:
                judge_content = judge_resp.choices[0].message.content
                if "<correct>True</correct>" in judge_content:
                    correctness = True
                else:
                    correctness = False

        arguments['correctness'] = correctness
        return arguments

    return arguments

# ---------------------------
# å¹¶å‘è°ƒåº¦å™¨ (ä¿æŒä¸å˜)
# ---------------------------
import contextlib
from concurrent.futures import ThreadPoolExecutor
from typing import Iterable, Tuple, Any, Callable

async def run_all(
    task_list: Iterable[Tuple[Callable[[Any], Any], Any]],
    concurrency: int = 2,
    progress: bool = False,
    return_exceptions: bool = False,
):
    loop = asyncio.get_running_loop()
    sem = asyncio.Semaphore(concurrency)

    with ThreadPoolExecutor(max_workers=concurrency) as executor:
        async def run_one(idx: int, func: Callable, arg: Any):
            async with sem:
                if asyncio.iscoroutinefunction(func):
                    res = await func(arg)
                else:
                    res = await loop.run_in_executor(executor, func, arg)
                return idx, res, None

        task_list = list(task_list)
        tasks = [asyncio.create_task(run_one(i, f, a)) for i, (f, a) in enumerate(task_list)]
        results = [None] * len(tasks)

        if progress:
            from tqdm import tqdm
            pbar = tqdm(total=len(tasks))
        else:
            pbar = None

        try:
            for fut in asyncio.as_completed(tasks):
                idx, res, err = await fut
                if err is None:
                    results[idx] = res
                else:
                    if return_exceptions:
                        results[idx] = err
                    else:
                        for t in tasks: t.cancel()
                        with contextlib.suppress(Exception):
                            await asyncio.gather(*tasks, return_exceptions=True)
                        raise err
                if pbar: pbar.update(1)
        finally:
            if pbar: pbar.close()
        return results


def run_single(e):
    doc_list = []
    code_list = []
    attempt_list = []
    problem = e['question']
    user_problem = problem
    answer = e['answer']
    all_tool_calls = []
    final_correct = False
    all_tool_responses = {}
    used_tools = []
    all_message_responses = {}
    
    for step in range(MAX_ROUNDS):
        cur_output_dir = os.path.join(my_output_dir,f"step_{step}")
        if not os.path.isdir(os.path.join(cur_output_dir,'tool_return')):
            try:
                os.makedirs(os.path.join(cur_output_dir,'tool_return'))
            except:
                pass
        tools = []
        doc_str = ''
        for doc_idx, doc in enumerate(doc_list):
            doc_str += f"Doc {doc_idx+1}: {doc[:1200]} ...\n\n"
        code_str = ''
        for code_idx, code_piece in enumerate(code_list):
            code_str += f"```python\n{code_piece['code']}\n```\n\n```output\n{code_piece['output']}\n```\n\n"
        attempt_str = ''
        for attempt_idx, attempt in enumerate(attempt_list):
            attempt_str += f"Attempt{attempt_idx+1} answer by {attempt['model']}: {attempt['answer']}\n"
        str_cut = cut_seq(seq=attempt_str,l=8000)
        attempt_str = str_cut['string_after_cut']
        if not attempt_str.startswith('Attempt') and len(attempt_str)>0:
            attempt_str = 'Attempt answer: '+attempt_str
        str_cut = cut_seq(seq=code_str+attempt_str,l=12000)
        code_attempt_str = str_cut['string_after_cut']
        code_attempt_str_len = str_cut['effective_length']
        if not code_attempt_str.startswith('```') and len(code_attempt_str)>0:
            code_attempt_str = '```\n'+code_attempt_str
        doc_flag = False
        problem_length = len(tokenizer(problem)['input_ids'])
        if code_attempt_str_len<27000-problem_length:
            if code_attempt_str:
                context_str = cut_seq(seq=doc_str+"\npython code and execution outputs:\n"+code_attempt_str,l=27000-problem_length)
            else:
                context_str = cut_seq(seq=doc_str,l=27000-problem_length)
            context_str = context_str['string_after_cut']
            if len(doc_str)>0:
                doc_flag = True
                context_str = 'Documents:\n'+context_str
        else:
            context_str = code_attempt_str
        removed_tool = None
        if len(used_tools)>1 and used_tools[-1]==used_tools[-2]:
            updated_tools = []
            removed_tool = used_tools[-1]
            for t in tools:
                if t['function']['name']!=used_tools[-1]:
                    updated_tools.append(t)
        else:
            updated_tools = tools
        cur_tool_set = [t['function']['name'] for t in updated_tools]

        # 2. è°ƒç”¨ Orchestrator
        chat = [
            {"role": "system", "content":"You are good at using tools"},
            {"role": "user", "content": f"Problem: {problem}\n\n{context_str}\n\nChoose an appropriate tool."}
        ]
        
        response = get_llm_response(
            model_alias='orchestrator-1', 
            messages=chat, 
            tools=raw_tools, 
            temperature=0.2,
            return_raw_response=True,
            max_length=12000
        )
        
        if isinstance(response, str):
            continue # å‡ºé”™é‡è¯•

        tool_calls = response.choices[0].message.tool_calls
        # è®°å½• æ¨¡å‹çš„æ€è€ƒå…¨è¿‡ç¨‹
        cache_tool_calls = []
        if tool_calls:
            for one_tool_call in tool_calls:
                t_name = one_tool_call.function.name
                try:
                    t_args = json.loads(one_tool_call.function.arguments)
                except:
                    t_args = {} # è§£æå¤±è´¥ç•™ç©ºï¼Œé˜²æ­¢æŠ¥é”™
                
                cache_tool_calls.append({
                    'tool_name': t_name,
                    'tool_arguments': t_args
                })
        
        # æ„é€  message_dict å¹¶å­˜å…¥æ€»è®°å½•
        message_dict = {
            'content': response.choices[0].message.content, # æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹æ–‡æœ¬
            'tool_calls': cache_tool_calls                  # è§£æåçš„å·¥å…·è°ƒç”¨åˆ—è¡¨
        }
        all_message_responses[f"turn_{step}_message"] = message_dict
        # -----------------------------------------------------------
        if not tool_calls or len(tool_calls) == 0:
            # æ²¡è°ƒå·¥å…·ï¼Œå¯èƒ½æ˜¯æƒ³ç›´æ¥å›ç­”ï¼Œæˆ–è€…å‡ºé”™äº†ã€‚è¿™é‡Œç®€å•continue
            continue
            
        # 3. è§£æå·¥å…·
        tool_call_list = []
        cur_tool_calls = []
        

        for one_tool_call in tool_calls:
            tool_name = one_tool_call.function.name
            try:
                tool_arguments = json.loads(one_tool_call.function.arguments)
            except:
                continue
            
            if tool_name not in ALL_TOOLS:
                continue
                
            tool_call_item = {
                'name': tool_name,
                'arguments': tool_arguments
            }
            cur_tool_calls.append(tool_call_item)
            expert_model_to_call = tool_arguments.get('model') 
            # å‡†å¤‡æ‰§è¡Œå‚æ•°
            call_tool_argument = {
                'tool': tool_name,
                'model': expert_model_to_call,
                'context_str': context_str,
                'cur_output_dir': cur_output_dir,
                'problem': user_problem,
                'answer': answer,
                'id': e.get('id', 'unknown'),
                'eid': e.get('eid', 0)
            }
            # æ„é€ æ‰§è¡Œå‡½æ•°å¯¹
            tool_call_list.append([call_tool, call_tool_argument])
            
            used_tools.append(tool_name)
            # åªè¦æœ‰ä¸€ä¸ªæ˜¯ answerï¼Œå°±åªæ‰§è¡Œè¿™ä¸€ä¸ª
            if tool_name == 'answer':
                break
        
        all_tool_calls.append(cur_tool_calls)
        
        if len(tool_call_list) == 0:
            continue

        # 4. æ‰§è¡Œå·¥å…·
        # run_all æ˜¯å¼‚æ­¥çš„ï¼Œrun_single æ˜¯åŒæ­¥çš„ï¼Œè¿™é‡Œç”¨ asyncio.run æ¡¥æ¥
        # æ³¨æ„ï¼šå› ä¸º tool_call_list æ˜¯ [[func, arg], ...]ï¼Œç¬¦åˆ run_all è¦æ±‚
        cur_responses = asyncio.run(run_all(tool_call_list))
        all_message_responses[f"turn_{step}_message"] = message_dict
        all_tool_responses[f"turn_{step}_response"] = cur_responses
        
        # 5. å¤„ç†ç»“æœ
        finish_flag = False
        for cur_response in cur_responses:
            if not cur_response: continue
            
            if cur_response['tool'] == 'enhance_reasoning':
                # [å…³é”®ä¿®æ”¹ 2] ç§»é™¤ len(...) > 0 çš„åˆ¤æ–­
                # æ— è®ºç»“æœæ˜¯ä»€ä¹ˆï¼Œéƒ½å¿…é¡»åŠ å…¥å†å²è®°å½•ï¼Œå¦åˆ™æ¨¡å‹ä¼šæ— é™é‡å¤
                code_content = cur_response.get('generated_code', '')
                exec_output = cur_response.get('exec_result', 'No result returned')
                
                # åªæœ‰å½“çœŸçš„æœ‰ä»£ç æ—¶æ‰è®°å½•
                if code_content:
                    code_list.append({'code': code_content, 'output': exec_output})
                    print(f"ğŸ“ Step {step}: Code execution recorded. Output length: {len(exec_output)}")
            
            elif cur_response['tool'] == 'search':
                for one_doc in cur_response.get('search_results_data', [])[::-1]:
                    if one_doc not in doc_list:
                        doc_list.append(one_doc)
            
            elif cur_response['tool'] == 'answer':
                final_correct = cur_response.get('correctness', False)
                finish_flag = True
                break
        
        if finish_flag:
            break

    return_dict = {
        'id': e['id'],
        'problem': problem,
        'all_tool_calls': all_tool_calls,
        'all_tool_responses': all_tool_responses,
        'all_message_response':all_message_responses,
        'answer': answer,
        'correct': final_correct
    }
    
    # ç»“æœå†™å…¥
    if not os.path.exists(my_output_dir):
        os.makedirs(my_output_dir, exist_ok=True)
    with open(os.path.join(my_output_dir, f"{e.get('id', 'unknown')}.json"), 'w') as f:
        json.dump(return_dict, f, indent=2)
    return return_dict

if __name__=='__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_name', type=str) # é»˜è®¤å€¼
    parser.add_argument('--output_dir', type=str)
    parser.add_argument('--example_file_path', type=str)
    parser.add_argument('--max_rounds', type=int, default=20) # å‡å°‘è½®æ•°çœé’±
    parser.add_argument('--basic_tools', action='store_true')
    args = parser.parse_args()

    # ä» LLM_API å¯¼å…¥çš„ MODEL_MAPPING ç”¨äºè¿™é‡Œçš„é€»è¾‘
    if args.basic_tools:
        keys = list(MODEL_MAPPING.keys())
        for k in keys:
            MODEL_MAPPING[k] = args.model_name

    MODEL_NAME = args.model_name
    my_output_dir = args.output_dir
    MAX_ROUNDS = args.max_rounds
    
    if not os.path.isdir(os.path.join(my_output_dir,'answer_cache')):
        os.makedirs(os.path.join(my_output_dir,'answer_cache'), exist_ok=True)
   
    # è¯»å–é¢˜ç›®
    with open(args.example_file_path) as f:
        lines = f.readlines()
    examples = []
    for eid,l in enumerate(lines):
        if not l.strip(): continue
        raw_example = json.loads(l)
        raw_example['eid'] = eid
        # ç¡®ä¿æœ‰ id å­—æ®µï¼Œå¦åˆ™æŠ¥é”™
        if 'id' not in raw_example:
            raw_example['id'] = f"test_{eid}"
        examples.append([run_single, raw_example])

    # è¿è¡Œ
    print(f"Starting evaluation on {len(examples)} examples...")
    tool_call_results = asyncio.run(run_all(examples, concurrency=2)) # æé«˜å¹¶å‘
    print("Evaluation finished.")